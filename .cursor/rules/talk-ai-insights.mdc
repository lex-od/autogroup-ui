---
description: 
globs: 
alwaysApply: true
---
Мы создаем веб-приложение для анализа звонков автомобильного холдинга с помошью ИИ.


Build a full-stack web application called **“CallInsight AI”**.

## 1. Goal
Create an analytics portal that lets non-technical business users upload or stream phone call recordings and instantly:
* transcribe them (multi-language, speaker-diarised);
* run AI analyses (sentiment, topic, intent, action-items);
* visualise trends, outliers, and agent performance over time.

## 2. Tech stack
* **Frontend:** React + Vite + Tailwind CSS, responsive (desktop & mobile).
* **Backend:** FastAPI (Python 3.12).  
  *Use Whisper-large-v3 for transcription; OpenAI GPT-4o for NLP tasks.*
* **Database:** PostgreSQL. Use SQLAlchemy + Alembic migrations.
* **Storage:** Store raw audio in S3-compatible bucket (e.g., MinIO) and save only the URL + metadata in DB.
* **Authentication:** Google OAuth2 + email/password fallback. Protect all API routes with JWT.

## 3. Data model
### Tables
| Table | Key fields |
|-------|------------|
| `users` | id, name, email, role |
| `calls` | id, user_id, path_to_audio, duration, created_at |
| `transcripts` | id, call_id, start_ms, end_ms, speaker, text |
| `analyses` | id, call_id, sentiment_score, topics (JSONB), summary, action_items (JSONB) |

Add appropriate FK constraints and indexes on `created_at`, `user_id`, and full-text search on `transcripts.text`.

## 4. Core API endpoints
* `POST /api/calls` – multipart upload (`wav`, `mp3`, `m4a` up to 60 min).  
  – Trigger background job: transcribe → analyse → persist results.
* `GET /api/calls?search=&date_from=&date_to=&sentiment=` – paginated list with filters.
* `GET /api/calls/{id}` – transcript + analysis JSON.
* `GET /api/dashboard/metrics` – aggregated metrics for charts.
* `POST /api/webhook/recording` – optional: accept URL from PBX system, download & process.

## 5. Background processing
Use Celery + Redis queue. Each task pipeline:
1. Download/ingest audio.  
2. Run Whisper (language auto-detect, `--diarize_speakers 2`).  
3. Store per-segment text with timecodes.  
4. Feed full transcript to GPT-4o to get:
   * call summary (≤ 120 words);
   * sentiment score `−1…1`;
   * key topics (<= 5, JSON array);
   * action items (bullet list, JSON array).
5. Persist to DB and emit WebSocket event to frontend.

## 6. Front-end UX
* **Dashboard page**  
  – KPI tiles: total calls, avg duration, % negative sentiment, trend sparkline.  
  – Time-series charts (Plotly): sentiment over time, top topics bar chart.  
* **Calls list page**  
  – Search bar (full-text), date & sentiment filters, sortable table.  
* **Call detail drawer**  
  – Audio player with waveform.  
  – Scrollable, colour-coded transcript (speaker A/B).  
  – “Key insights” panel (summary, topics chips, action-items checklist).  
* **Export** button: download transcript & analysis as PDF or send to Google Sheets via API.

## 7. Integrations & extensibility
* Optional Google Sheets export (Service Account credentials env-var).
* GA4 event push when a call with negative sentiment is viewed.
* Webhook template to push flagged calls to Slack.

## 8. Non-functional
* Target < 3 s TTFB for GET requests.  
* Enforce file-size limit 100 MB.  
* Unit + integration tests (pytest) for all API endpoints and processing logic.  
* CI: GitHub Actions → run tests → deploy to Lazy Cloud on main branch.

## 9. Deliverables
* Working production deployment.  
* `/docs` (FastAPI-Swagger) auto-generated API reference.  

* Seed script that creates a demo user and imports two sample recordings.